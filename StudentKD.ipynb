{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15929e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "num_classes = 7\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "temperature = 3.0\n",
    "alpha_min, alpha_max = 0.3, 0.5\n",
    "beta_min, beta_max = 0.3, 0.5\n",
    "lambda_min, lambda_max = 0.1, 0.4\n",
    "train_path = '/code/MyCode/AUG/HAM10000/train_dir'\n",
    "teacher_probs_path = 'teacher_hybrid_probs.npy'\n",
    "teacher_lrp_path = 'LRP_hybrid.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input)\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_probs = np.load(teacher_probs_path)\n",
    "teacher_lrp = np.load(teacher_lrp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a65fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg')\n",
    "x = base.output\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "student = Model(inputs=base.input, outputs=output)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd05bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lrp(model, X_batch):\n",
    "    return np.abs(X_batch).mean(axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_gen)\n",
    "for epoch in range(epochs):\n",
    "    alpha = alpha_min + (epoch/epochs)*(alpha_max-alpha_min)\n",
    "    beta  = beta_min + (epoch/epochs)*(beta_max-beta_min)\n",
    "    lambda_t = lambda_min + (epoch/epochs)*(lambda_max-lambda_min)\n",
    "    s = alpha+beta+lambda_t\n",
    "    alpha, beta, lambda_t = alpha/s, beta/s, lambda_t/s\n",
    "    for batch_idx in range(steps_per_epoch):\n",
    "        X_batch, y_batch = train_gen.next()\n",
    "        start_idx = batch_idx*batch_size\n",
    "        end_idx = start_idx+X_batch.shape[0]\n",
    "        teacher_batch = teacher_probs[start_idx:end_idx]\n",
    "        teacher_lrp_batch = teacher_lrp[start_idx:end_idx]\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = student(X_batch, training=True)\n",
    "            L_CE = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_batch, y_pred))\n",
    "            y_pred_soft = tf.nn.softmax(y_pred/temperature)\n",
    "            teacher_soft = tf.nn.softmax(teacher_batch/temperature)\n",
    "            L_KD = tf.reduce_mean(tf.keras.losses.KLDivergence()(teacher_soft, y_pred_soft))*(temperature**2)\n",
    "            student_lrp_batch = compute_lrp(student, X_batch)\n",
    "            L_importance = tf.reduce_mean(tf.abs(student_lrp_batch - teacher_lrp_batch))\n",
    "            L_total = alpha*L_CE + beta*L_KD + lambda_t*L_importance\n",
    "        grads = tape.gradient(L_total, student.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, student.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.save(\"student_EfficientNetB0_KD_LRP.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/code/MyCode/AUG/HAM10000/test_dir'\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "steps_test = len(test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_preds = []\n",
    "for _ in range(steps_test):\n",
    "    X_batch, _ = test_gen.next()\n",
    "    preds_batch = student.predict(X_batch, verbose=0)\n",
    "    student_preds.append(preds_batch)\n",
    "student_preds = np.vstack(student_preds)\n",
    "np.save(\"student_predictions.npy\", student_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_lrp_test = []\n",
    "test_gen.reset()\n",
    "for _ in range(steps_test):\n",
    "    X_batch, _ = test_gen.next()\n",
    "    lrp_batch = compute_lrp(student, X_batch)\n",
    "    student_lrp_test.append(lrp_batch)\n",
    "student_lrp_test = np.vstack(student_lrp_test)\n",
    "np.save(\"student_LRP.npy\", student_lrp_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da69c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_lrp_test(X_batch, student_lrp_batch, idx=0):\n",
    "    img = X_batch[idx]\n",
    "    s_lrp = student_lrp_batch[idx].squeeze()\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2, figsize=(8,4))\n",
    "    axes[0].imshow((img - img.min()) / (img.max() - img.min()))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(s_lrp, cmap='hot')\n",
    "    axes[1].set_title(\"Student LRP\")\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "X_batch, _ = test_gen.next()\n",
    "show_lrp_test(X_batch, student_lrp_test, idx=0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
