{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer,InputSpec\n",
    "import keras.layers as kl\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from  matplotlib import pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import concatenate,Dense, Conv2D, MaxPooling2D, Flatten,Input,Activation,add,AveragePooling2D,BatchNormalization,Dropout\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/code/MyCode/AUG/HAM10000/train.csv')\n",
    "test_df = pd.read_csv('/code/MyCode/AUG/HAM10000/test.csv')\n",
    "targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "train_path = '/code/MyCode/AUG/HAM10000/train_dir'\n",
    "test_path = '/code/MyCode/AUG/HAM10000/test_dir'\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_resnet_v2.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Batches: \n",
      "Found 55115 images belonging to 7 classes.\n",
      "\n",
      "Test Batches: \n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 299\n",
    "print(\"\\nTrain Batches: \")\n",
    "train_batches = datagen.flow_from_directory(directory=train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "print(\"\\nTest Batches: \")\n",
    "test_batches =datagen.flow_from_directory(test_path,\n",
    "                                           target_size=(image_size,image_size),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soft Attention\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer,InputSpec\n",
    "import keras.layers as kl\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class SoftAttention(Layer):\n",
    "    def __init__(self,ch,m,concat_with_x=False,aggregate=False,**kwargs):\n",
    "        self.channels=int(ch)\n",
    "        self.multiheads = m\n",
    "        self.aggregate_channels = aggregate\n",
    "        self.concat_input_with_scaled = concat_with_x\n",
    "\n",
    "\n",
    "        super(SoftAttention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "\n",
    "        self.i_shape = input_shape\n",
    "\n",
    "        kernel_shape_conv3d = (self.channels, 3, 3) + (1, self.multiheads) # DHWC\n",
    "\n",
    "        self.out_attention_maps_shape = input_shape[0:1]+(self.multiheads,)+input_shape[1:-1]\n",
    "\n",
    "        if self.aggregate_channels==False:\n",
    "\n",
    "            self.out_features_shape = input_shape[:-1]+(input_shape[-1]+(input_shape[-1]*self.multiheads),)\n",
    "        else:\n",
    "            if self.concat_input_with_scaled:\n",
    "                self.out_features_shape = input_shape[:-1]+(input_shape[-1]*2,)\n",
    "            else:\n",
    "                self.out_features_shape = input_shape\n",
    "\n",
    "\n",
    "        self.kernel_conv3d = self.add_weight(shape=kernel_shape_conv3d,\n",
    "                                        initializer='he_uniform',\n",
    "                                        name='kernel_conv3d')\n",
    "        self.bias_conv3d = self.add_weight(shape=(self.multiheads,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_conv3d')\n",
    "\n",
    "        super(SoftAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        exp_x = K.expand_dims(x,axis=-1)\n",
    "\n",
    "        c3d = K.conv3d(exp_x,\n",
    "                     kernel=self.kernel_conv3d,\n",
    "                     strides=(1,1,self.i_shape[-1]), padding='same', data_format='channels_last')\n",
    "        conv3d = K.bias_add(c3d,\n",
    "                        self.bias_conv3d)\n",
    "        conv3d = kl.Activation('relu')(conv3d)\n",
    "\n",
    "        conv3d = K.permute_dimensions(conv3d,pattern=(0,4,1,2,3))\n",
    "\n",
    "\n",
    "        conv3d = K.squeeze(conv3d, axis=-1)\n",
    "        conv3d = K.reshape(conv3d,shape=(-1, self.multiheads ,self.i_shape[1]*self.i_shape[2]))\n",
    "\n",
    "        softmax_alpha = K.softmax(conv3d, axis=-1)\n",
    "        softmax_alpha = kl.Reshape(target_shape=(self.multiheads, self.i_shape[1],self.i_shape[2]))(softmax_alpha)\n",
    "\n",
    "\n",
    "        if self.aggregate_channels==False:\n",
    "            exp_softmax_alpha = K.expand_dims(softmax_alpha, axis=-1)\n",
    "            exp_softmax_alpha = K.permute_dimensions(exp_softmax_alpha,pattern=(0,2,3,1,4))\n",
    "\n",
    "            x_exp = K.expand_dims(x,axis=-2)\n",
    "\n",
    "            u = kl.Multiply()([exp_softmax_alpha, x_exp])\n",
    "\n",
    "            u = kl.Reshape(target_shape=(self.i_shape[1],self.i_shape[2],u.shape[-1]*u.shape[-2]))(u)\n",
    "\n",
    "        else:\n",
    "            exp_softmax_alpha = K.permute_dimensions(softmax_alpha,pattern=(0,2,3,1))\n",
    "\n",
    "            exp_softmax_alpha = K.sum(exp_softmax_alpha,axis=-1)\n",
    "\n",
    "            exp_softmax_alpha = K.expand_dims(exp_softmax_alpha, axis=-1)\n",
    "\n",
    "            u = kl.Multiply()([exp_softmax_alpha, x])\n",
    "\n",
    "        if self.concat_input_with_scaled:\n",
    "            o = kl.Concatenate(axis=-1)([u,x])\n",
    "        else:\n",
    "            o = u\n",
    "\n",
    "        return [o, softmax_alpha]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [self.out_features_shape, self.out_attention_maps_shape]\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(SoftAttention,self).get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "irv2 = tf.keras.applications.InceptionResNetV2(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classifier_activation=\"softmax\",\n",
    "\n",
    ")\n",
    "\n",
    "# Excluding the last 28 layers of the model.\n",
    "conv = irv2.layers[-28].output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SOFT ATTENTION\n",
    "attention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(conv.shape[-1]),name='soft_attention')(conv)\n",
    "attention_layer=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer))\n",
    "conv=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(conv))\n",
    "\n",
    "conv = concatenate([conv,attention_layer])\n",
    "conv  = Activation('relu')(conv)\n",
    "conv = Dropout(0.5)(conv)\n",
    "\n",
    "\n",
    "output = Flatten()(conv)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model(inputs=irv2.input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model.load_weights(\"/code/MyCode/AUG/Aug-Att/IRV2+SA.hdf5\")\n",
    "predictions_IRV2 = model.predict(test_batches, steps=len(test_df)/batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DENSENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Batches: \n",
      "Found 55115 images belonging to 7 classes.\n",
      "\n",
      "Test Batches: \n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "print(\"\\nTrain Batches: \")\n",
    "train_batches = datagen.flow_from_directory(directory=train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "print(\"\\nTest Batches: \")\n",
    "test_batches =datagen.flow_from_directory(test_path,\n",
    "                                           target_size=(image_size,image_size),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = tf.keras.applications.DenseNet201(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "\n",
    ")\n",
    "# Exclude the last 28 layers of the model.\n",
    "conv = densenet.layers[-28].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SOFT ATTENTION\n",
    "attention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(conv.shape[-1]),name='soft_attention')(conv)\n",
    "attention_layer=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer))\n",
    "conv=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(conv))\n",
    "\n",
    "conv = concatenate([conv,attention_layer])\n",
    "conv  = Activation('relu')(conv)\n",
    "conv = Dropout(0.5)(conv)\n",
    "\n",
    "\n",
    "output = Flatten()(conv)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model(inputs=densenet.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model.load_weights(\"/code/MyCode/AUG/Aug-Att/Densenet201+SA.hdf5\")\n",
    "predictions_Densenet = model.predict(test_batches, steps=len(test_df)/batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Batches: \n",
      "Found 55115 images belonging to 7 classes.\n",
      "\n",
      "Test Batches: \n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "print(\"\\nTrain Batches: \")\n",
    "train_batches = datagen.flow_from_directory(directory=train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "print(\"\\nTest Batches: \")\n",
    "test_batches =datagen.flow_from_directory(test_path,\n",
    "                                           target_size=(image_size,image_size),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    ")\n",
    "\n",
    "# Exclude the last 3 layers of the model.\n",
    "conv = resnet.layers[-3].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(conv.shape[-1]),name='soft_attention')(conv)\n",
    "attention_layer=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer))\n",
    "conv=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(conv))\n",
    "\n",
    "conv = concatenate([conv,attention_layer])\n",
    "conv  = Activation('relu')(conv)\n",
    "conv = Dropout(0.5)(conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "output = GlobalAveragePooling2D()(conv)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model(inputs=resnet.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model.load_weights(\"/code/MyCode/AUG/Aug-Att/ResNet50+SA.hdf5\")\n",
    "predictions_Resnet50 = model.predict(test_batches, steps=len(test_df)/batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Batches: \n",
      "Found 55115 images belonging to 7 classes.\n",
      "\n",
      "Test Batches: \n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "print(\"\\nTrain Batches: \")\n",
    "train_batches = datagen.flow_from_directory(directory=train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "print(\"\\nTest Batches: \")\n",
    "test_batches =datagen.flow_from_directory(test_path,\n",
    "                                           target_size=(image_size,image_size),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MainInput=Input(shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convlayer1(input_value):\n",
    "  conv1=Conv2D(filters=64, kernel_size=(3,3), strides=(2,2),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  conv1=BatchNormalization()(conv1)\n",
    "  pool1=MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "  return pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convlayer2(input_value):\n",
    "  conv2=Conv2D(filters=64, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  conv2=BatchNormalization()(conv2)\n",
    "  conv2=Conv2D(filters=64, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(conv2)\n",
    "  conv2=BatchNormalization()(conv2)\n",
    "\n",
    "  resnet=add([input_value,conv2])\n",
    "  resnet=Activation(\"relu\")(resnet)\n",
    "  return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convlayer3(input_value):\n",
    "  conv3=Conv2D(filters=128, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  conv3=BatchNormalization()(conv3)\n",
    "  conv3=Conv2D(filters=128, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(conv3)\n",
    "  conv3=BatchNormalization()(conv3)\n",
    "\n",
    "\n",
    "  skip=Conv2D(filters=128, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  skip=BatchNormalization()(skip)\n",
    "\n",
    "  resnet=add([skip,conv3])\n",
    "  resnet=Activation(\"relu\")(resnet)\n",
    "  return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convlayer4(input_value):\n",
    "  conv4=Conv2D(filters=256, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  conv4=BatchNormalization()(conv4)\n",
    "  conv4=Conv2D(filters=256, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(conv4)\n",
    "  conv4=BatchNormalization()(conv4)\n",
    "\n",
    "\n",
    "  skip=Conv2D(filters=256, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  skip=BatchNormalization()(skip)\n",
    "\n",
    "  resnet=add([skip,conv4])\n",
    "  resnet=Activation(\"relu\")(resnet)\n",
    "  return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convlayer5(input_value):\n",
    "  conv5=Conv2D(filters=512, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  conv5=BatchNormalization()(conv5)\n",
    "  conv5=Conv2D(filters=512, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(conv5)\n",
    "  conv5=BatchNormalization()(conv5)\n",
    "\n",
    "\n",
    "  skip=Conv2D(filters=512, kernel_size=(3,3),activation=\"relu\",padding=\"same\")(input_value)\n",
    "  skip=BatchNormalization()(skip)\n",
    "\n",
    "  resnet=add([skip,conv5])\n",
    "  resnet=Activation(\"relu\")(resnet)\n",
    "  return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1=convlayer1(MainInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "block2=convlayer2(block1)\n",
    "for i in range(0,2):\n",
    "  block2=convlayer2(block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool=MaxPooling2D(pool_size=(2,2), padding='same')(block2)\n",
    "block3=convlayer3(maxpool)\n",
    "\n",
    "for i in range(0,3):\n",
    "  block3=convlayer3(block3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attention_layer2,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(block3.shape[-1]),name='soft_attention')(block3)\n",
    "attention_layer2=MaxPooling2D(pool_size=(2,2), padding='same')(attention_layer2)\n",
    "maxpool=MaxPooling2D(pool_size=(2,2), padding='same')(block3)\n",
    "\n",
    "concat2=concatenate([maxpool,attention_layer2])\n",
    "conv = Activation(\"relu\")(concat2)\n",
    "conv= Dropout(0.5)(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "block4=convlayer4(conv)\n",
    "for i in range(0,5):\n",
    "  block4=convlayer4(block4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool=MaxPooling2D(pool_size=(2,2), padding='same')(block4)\n",
    "block5=convlayer5(maxpool)\n",
    "for i in range(0,2):\n",
    "  block5=convlayer5(block5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = GlobalAveragePooling2D()(block5)\n",
    "output = Dense(7, activation='softmax')(output)\n",
    "model = Model(inputs=MainInput, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model.load_weights(\"/code/MyCode/AUG/Aug-Att/ResNet34+SA.hdf5\")\n",
    "predictions_Resnet34 = model.predict(test_batches, steps=len(test_df)/batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Batches: \n",
      "Found 55115 images belonging to 7 classes.\n",
      "\n",
      "Test Batches: \n",
      "Found 2003 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "print(\"\\nTrain Batches: \")\n",
    "train_batches = datagen.flow_from_directory(directory=train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "print(\"\\nTest Batches: \")\n",
    "test_batches =datagen.flow_from_directory(test_path,\n",
    "                                           target_size=(image_size,image_size),\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MainInput=Input(shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(Conv2D(filters=64,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(MainInput))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=64,kernel_size=(1,1), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(MaxPooling2D(strides=(2, 2),padding=\"same\")(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(Conv2D(filters=128,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=128,kernel_size=(1,1), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(MaxPooling2D()(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(Conv2D(filters=256,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=256,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=256,kernel_size=(1,1), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(MaxPooling2D()(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(Conv2D(filters=512,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=512,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=512,kernel_size=(1,1), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(conv.shape[-1]),name='soft_attention')(conv)\n",
    "attention_layer=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer))\n",
    "conv=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(conv))\n",
    "\n",
    "conv = concatenate([conv,attention_layer])\n",
    "conv=Activation(\"relu\")(conv)\n",
    "conv= Dropout(0.5)(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(Conv2D(filters=512,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=512,kernel_size=(3,3), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))\n",
    "conv=(Conv2D(filters=512,kernel_size=(1,1), activation=\"relu\",padding=\"same\",kernel_initializer='he_normal')(conv))\n",
    "conv=(BatchNormalization()(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(MaxPooling2D(pool_size=(4, 4),padding=\"same\")(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=(Flatten()(conv))\n",
    "conv=(Dense(4096,activation=\"relu\")(conv))\n",
    "conv=(Dense(4096,activation=\"relu\")(conv))\n",
    "conv=(Dense(7, activation=\"softmax\")(conv))\n",
    "\n",
    "model = Model(inputs=MainInput, outputs=conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model.load_weights(\"/code/MyCode/AUG/Aug-Att/vgg16+SA.hdf5\")\n",
    "predictions_VGG16 = model.predict(test_batches, steps=len(test_df)/batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_IRV2      = np.load(\"predictions_IRV2.npy\")\n",
    "predictions_Densenet  = np.load(\"predictions_Densenet.npy\")\n",
    "predictions_Resnet50  = np.load(\"predictions_Resnet50.npy\")\n",
    "predictions_Resnet34  = np.load(\"predictions_Resnet34.npy\")\n",
    "predictions_VGG16     = np.load(\"predictions_VGG16.npy\")\n",
    "\n",
    "y_true = np.load(\"y_true.npy\")\n",
    "\n",
    "targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "\n",
    "all_preds = [predictions_IRV2, predictions_Densenet, predictions_Resnet50,\n",
    "             predictions_Resnet34, predictions_VGG16]\n",
    "\n",
    "N, C = predictions_IRV2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mass(m):\n",
    "    s = np.sum(m)\n",
    "    return m / s if s != 0 else m\n",
    "\n",
    "def ds_combine_two(m1, m2):\n",
    "    C = m1.shape[0]\n",
    "    m_comb = np.zeros(C)\n",
    "    conflict = 0.0\n",
    "\n",
    "    for i in range(C):\n",
    "        for j in range(C):\n",
    "            if i == j:\n",
    "                m_comb[i] += m1[i] * m2[j]\n",
    "            else:\n",
    "                conflict += m1[i] * m2[j]\n",
    "\n",
    "    if conflict < 1.0:\n",
    "        m_comb = m_comb / (1.0 - conflict)\n",
    "    else:\n",
    "        m_comb = (m1 + m2) / 2.0\n",
    "\n",
    "    return normalize_mass(m_comb)\n",
    "\n",
    "def ds_fusion(predictions_list):\n",
    "    fused = predictions_list[0].copy()\n",
    "    for preds in predictions_list[1:]:\n",
    "        for i in range(fused.shape[0]):\n",
    "            fused[i] = ds_combine_two(fused[i], preds[i])\n",
    "    return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_ds = ds_fusion(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_array = np.stack(all_preds, axis=0)  \n",
    "disagreement = np.std(all_preds_array, axis=0).mean(axis=1)  \n",
    "\n",
    "threshold = disagreement.mean() + disagreement.std()\n",
    "print(f\"Disagreement threshold: {threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_update(prior, predictions_list, eps=1e-8):\n",
    "   \n",
    "    log_post = np.log(prior + eps)\n",
    "    for pred in predictions_list:\n",
    "        log_post += np.log(pred + eps)\n",
    "    posterior = np.exp(log_post)\n",
    "    posterior /= posterior.sum()\n",
    "    return posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_hybrid = np.zeros_like(teacher_ds)\n",
    "\n",
    "for i in range(N):\n",
    "    if disagreement[i] <= threshold:\n",
    "        # Low disagreement → keep DS output\n",
    "        teacher_hybrid[i] = teacher_ds[i]\n",
    "    else:\n",
    "        # High disagreement → use DS as prior + Bayesian update\n",
    "        sample_preds = [pred[i] for pred in all_preds]\n",
    "        teacher_hybrid[i] = bayesian_update(teacher_ds[i], sample_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"teacher_hybrid_probs.npy\", teacher_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hybrid = np.argmax(teacher_hybrid, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred_hybrid, target_names=targetnames))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_hybrid)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Hybrid DS + Bayesian Teacher – Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(7), targetnames, rotation=45)\n",
    "plt.yticks(range(7), targetnames)\n",
    "\n",
    "thresh_cm = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh_cm else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mass_map(m):\n",
    "    s = np.sum(m)\n",
    "    return m / s if s != 0 else m\n",
    "\n",
    "def ds_combine_two_map(m1, m2):\n",
    "    \"\"\"\n",
    "    Apply DS fusion on 3D or 4D importance maps (H,W,C)\n",
    "    \"\"\"\n",
    "    m_comb = np.zeros_like(m1)\n",
    "    conflict = 0.0\n",
    "    \n",
    "    flat1 = m1.flatten()\n",
    "    flat2 = m2.flatten()\n",
    "    \n",
    "    for i in range(len(flat1)):\n",
    "        for j in range(len(flat2)):\n",
    "            if i == j:\n",
    "                m_comb.flat[i] += flat1[i] * flat2[j]\n",
    "            else:\n",
    "                conflict += flat1[i] * flat2[j]\n",
    "\n",
    "    if conflict < 1.0:\n",
    "        m_comb = m_comb / (1.0 - conflict)\n",
    "    else:\n",
    "        m_comb = (m1 + m2) / 2.0\n",
    "    \n",
    "    return normalize_mass_map(m_comb)\n",
    "\n",
    "def ds_fusion_map(lrp_list):\n",
    "    fused = lrp_list[0].copy()\n",
    "    for lrp in lrp_list[1:]:\n",
    "        for i in range(fused.shape[0]):\n",
    "            fused[i] = ds_combine_two_map(fused[i], lrp[i])\n",
    "    return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_ds = ds_fusion_map(all_lrp)\n",
    "print(\"LRP DS fusion done, shape:\", lrp_ds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lrp_array = np.stack(all_lrp, axis=0)  # shape: (num_models, N, H, W, C)\n",
    "disagreement = np.std(all_lrp_array, axis=0).mean(axis=(1,2,3))  # mean over H,W,C\n",
    "\n",
    "threshold = disagreement.mean() + disagreement.std()\n",
    "print(f\"LRP disagreement threshold: {threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_update_map(prior_map, maps_list, eps=1e-8):\n",
    "    log_post = np.log(prior_map + eps)\n",
    "    for m in maps_list:\n",
    "        log_post += np.log(m + eps)\n",
    "    posterior = np.exp(log_post)\n",
    "    posterior /= posterior.sum()\n",
    "    return posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_hybrid = np.zeros_like(lrp_ds)\n",
    "\n",
    "for i in range(N):\n",
    "    if disagreement[i] <= threshold:\n",
    "        # low disagreement → keep DS map\n",
    "        lrp_hybrid[i] = lrp_ds[i]\n",
    "    else:\n",
    "        # high disagreement → use DS as prior + Bayesian update\n",
    "        sample_maps = [lrp[i] for lrp in all_lrp]\n",
    "        lrp_hybrid[i] = bayesian_update_map(lrp_ds[i], sample_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"LRP_hybrid.npy\", lrp_hybrid)\n",
    "print(\"Saved LRP_hybrid.npy with shape:\", lrp_hybrid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/code/MyCode/AUG/HAM10000/test_dir'\n",
    "\n",
    "test_batches = datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "steps_test = len(test_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_teacher = model.predict(test_batches, steps=steps_test, verbose=1)\n",
    "np.save(\"teacher_predictions.npy\", predictions_teacher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_lrp_test = []\n",
    "test_batches.reset()\n",
    "\n",
    "for i in range(steps_test):\n",
    "    X_batch, _ = test_batches.next()\n",
    "    lrp_batch = compute_lrp(model, X_batch)  # LRP-0 for teacher\n",
    "    teacher_lrp_test.append(lrp_batch)\n",
    "\n",
    "teacher_lrp_test = np.vstack(teacher_lrp_test)\n",
    "np.save(\"teacher_LRP.npy\", teacher_lrp_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_batch, _ = test_batches.next()\n",
    "idx = 0\n",
    "\n",
    "img = X_batch[idx]\n",
    "lrp = teacher_lrp_test[idx].squeeze()\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "axes[0].imshow((img - img.min()) / (img.max() - img.min()))\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(lrp, cmap='hot')\n",
    "axes[1].set_title(\"Teacher LRP\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Normalizing predictions...\n",
      "Step 2: Creating feature matrix...\n",
      "Step 3: Preprocessing features...\n",
      "Step 4: Processing raw predictions for t-SNE...\n",
      "Creating Plot 1: Raw predictions colored by predicted class...\n",
      "Creating Plot 2: Raw predictions colored by conflict...\n",
      "Calculating conflict data for bar chart...\n",
      "Creating Plot 3: Average conflict by class...\n",
      "All three plots saved as separate PDF files:\n",
      "1. raw_predictions_by_class.pdf\n",
      "2. raw_predictions_by_conflict.pdf\n",
      "3. average_conflict_by_class.pdf\n"
     ]
    }
   ],
   "source": [
    "class_names = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC']\n",
    "n_classes = len(class_names)\n",
    "\n",
    "teacher_hybrid = np.load(\"teacher_hybrid_probs.npy\")  \n",
    "n_samples = teacher_hybrid.shape[0]\n",
    "\n",
    "predictions_IRV2      = np.load(\"predictions_IRV2.npy\")\n",
    "predictions_Densenet  = np.load(\"predictions_Densenet.npy\")\n",
    "predictions_Resnet50  = np.load(\"predictions_Resnet50.npy\")\n",
    "predictions_Resnet34  = np.load(\"predictions_Resnet34.npy\")\n",
    "predictions_VGG16     = np.load(\"predictions_VGG16.npy\")\n",
    "\n",
    "all_preds = [predictions_IRV2, predictions_Densenet, predictions_Resnet50,\n",
    "             predictions_Resnet34, predictions_VGG16]\n",
    "\n",
    "def normalize_predictions(pred):\n",
    "    s = pred.sum(axis=1, keepdims=True)\n",
    "    s[s==0] = 1\n",
    "    return pred / s\n",
    "\n",
    "teacher_hybrid = normalize_predictions(teacher_hybrid)\n",
    "ensemble_mean = np.mean(all_preds, axis=0)  \n",
    "ensemble_std = np.std(all_preds, axis=0)    \n",
    "\n",
    "\n",
    "predicted_classes = np.argmax(np.stack(all_preds, axis=0), axis=2)\n",
    "class_disagreement = np.mean(predicted_classes != predicted_classes[0], axis=0)\n",
    "\n",
    "# t-SNE \n",
    "all_predictions = np.concatenate(all_preds, axis=1)  \n",
    "\n",
    "feature_sets = {\n",
    "    'raw_predictions': all_predictions,\n",
    "    'ensemble_stats': np.concatenate([ensemble_mean, ensemble_std], axis=1),\n",
    "    'disagreement_features': np.concatenate([ensemble_mean, ensemble_std, class_disagreement.reshape(-1,1)], axis=1),\n",
    "    'pca_reduced': None\n",
    "}\n",
    "\n",
    "pca = PCA(n_components=min(50, all_predictions.shape[1]))\n",
    "feature_sets['pca_reduced'] = pca.fit_transform(all_predictions)\n",
    "\n",
    "features = all_predictions\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "optimal_perplexity = min(30, max(5, n_samples // 50))\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=optimal_perplexity,\n",
    "    n_iter=1000,\n",
    "    learning_rate=200,\n",
    "    random_state=42,\n",
    "    init='pca',\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "tsne_results = tsne.fit_transform(features_scaled)\n",
    "\n",
    "ensemble_pred_class = np.argmax(ensemble_mean, axis=1)\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(10,8))\n",
    "cmap = plt.get_cmap(\"tab10\", n_classes)\n",
    "scatter1 = ax1.scatter(tsne_results[:,0], tsne_results[:,1],\n",
    "                       c=ensemble_pred_class, cmap=cmap, alpha=0.7, s=20)\n",
    "ax1.set_xlabel('t-SNE 1')\n",
    "ax1.set_ylabel('t-SNE 2')\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_ticks(range(n_classes))\n",
    "cbar1.set_ticklabels(class_names)\n",
    "cbar1.set_label('Predicted Class')\n",
    "plt.savefig('raw_predictions_by_class.pdf', format='pdf', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "original_reds = plt.get_cmap('Reds')(np.linspace(0,1,256))\n",
    "darker_reds = original_reds[64:]\n",
    "new_reds_cmap = ListedColormap(darker_reds)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(10,8))\n",
    "scatter2 = ax2.scatter(tsne_results[:,0], tsne_results[:,1],\n",
    "                       c=class_disagreement, cmap=new_reds_cmap, alpha=0.7, s=20,\n",
    "                       vmin=0.3 * np.max(class_disagreement))\n",
    "ax2.set_xlabel('t-SNE 1')\n",
    "ax2.set_ylabel('t-SNE 2')\n",
    "cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "cbar2.set_label('Conflict')\n",
    "plt.savefig('raw_predictions_by_conflict.pdf', format='pdf', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "class_std_per_sample = ensemble_std\n",
    "max_conflict_class = np.argmax(class_std_per_sample, axis=1)\n",
    "max_conflict_value = np.max(class_std_per_sample, axis=1)\n",
    "\n",
    "avg_conflict_per_class = []\n",
    "for i in range(n_classes):\n",
    "    mask = max_conflict_class == i\n",
    "    avg_conflict_per_class.append(np.mean(max_conflict_value[mask]) if np.sum(mask) > 0 else 0)\n",
    "\n",
    "fig3, ax3 = plt.subplots(figsize=(10,6))\n",
    "bars = ax3.bar(range(n_classes), avg_conflict_per_class,\n",
    "               color=plt.cm.tab10(np.linspace(0,1,n_classes)))\n",
    "ax3.set_xlabel('Class')\n",
    "ax3.set_ylabel('Average Conflict')\n",
    "ax3.set_xticks(range(n_classes))\n",
    "ax3.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.savefig('average_conflict_by_class.pdf', format='pdf', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"All three plots saved as PDF files.\")\n",
    "print(\"1. raw_predictions_by_class.pdf\")\n",
    "print(\"2. raw_predictions_by_conflict.pdf\")\n",
    "print(\"3. average_conflict_by_class.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
