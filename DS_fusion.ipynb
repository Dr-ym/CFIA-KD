{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2447d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d55912",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_IRV2      = np.load(\"predictions_IRV2.npy\")\n",
    "predictions_Densenet  = np.load(\"predictions_Densenet.npy\")\n",
    "predictions_Resnet50  = np.load(\"predictions_Resnet50.npy\")\n",
    "predictions_Resnet34  = np.load(\"predictions_Resnet34.npy\")\n",
    "predictions_VGG16     = np.load(\"predictions_VGG16.npy\")\n",
    "\n",
    "y_true = np.load(\"y_true.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.stack([\n",
    "    predictions_IRV2,\n",
    "    predictions_Densenet,\n",
    "    predictions_Resnet50,\n",
    "    predictions_Resnet34,\n",
    "    predictions_VGG16\n",
    "], axis=0)\n",
    "\n",
    "print(predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_to_belief(prob):\n",
    "    \"\"\"\n",
    "    prob: (num_classes,)\n",
    "    returns belief mass (same shape)\n",
    "    \"\"\"\n",
    "    prob = np.clip(prob, 1e-8, 1.0)\n",
    "    return prob / np.sum(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_combine(belief1, belief2):\n",
    "    \"\"\"\n",
    "    belief1, belief2: (num_classes,)\n",
    "    \"\"\"\n",
    "    num_classes = belief1.shape[0]\n",
    "    \n",
    "    combined = np.zeros(num_classes)\n",
    "    conflict = 0.0\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        combined[c] = belief1[c] * belief2[c]\n",
    "\n",
    "    # Conflict = sum of mass assigned to empty intersections\n",
    "    conflict = np.sum(belief1 * (1 - belief2))\n",
    "\n",
    "    # Normalize\n",
    "    combined = combined / (1.0 - conflict + 1e-8)\n",
    "\n",
    "    return combined, conflict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ds_fuse_models(model_probs):\n",
    "    \"\"\"\n",
    "    model_probs: (num_models, num_classes)\n",
    "    \"\"\"\n",
    "    belief = prob_to_belief(model_probs[0])\n",
    "\n",
    "    total_conflict = 0.0\n",
    "\n",
    "    for i in range(1, model_probs.shape[0]):\n",
    "        belief_i = prob_to_belief(model_probs[i])\n",
    "        belief, conflict = ds_combine(belief, belief_i)\n",
    "        total_conflict += conflict\n",
    "\n",
    "    return belief, total_conflict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a86e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = predictions.shape[1]\n",
    "num_classes = predictions.shape[2]\n",
    "\n",
    "teacher_probs = np.zeros((num_samples, num_classes))\n",
    "conflicts = np.zeros(num_samples)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    fused, conflict = ds_fuse_models(predictions[:, i, :])\n",
    "    teacher_probs[i] = fused\n",
    "    conflicts[i] = conflict\n",
    "\n",
    "print(\"Teacher predictions shape:\", teacher_probs.shape)\n",
    "print(\"Mean conflict:\", np.mean(conflicts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_batches.classes\n",
    "y_pred = np.argmax(teacher_probs, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Teacher Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=targetnames))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
